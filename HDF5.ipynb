{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group1\n",
      "group1/subgroup1\n",
      "group1/subgroup1/subgroup11\n",
      "group1/subgroup1/subgroup11/sensors\n",
      "group1/train\n",
      "my_data\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49]\n",
      "name: main group\n",
      "author: ksopyla\n",
      "KeysView(<HDF5 file \"file.hdf5\" (mode r)>)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    " \n",
    "def printname(name):\n",
    "        print (name)\n",
    " \n",
    "with h5py.File('file.hdf5','w') as hf:\n",
    "    \n",
    "    #cretea new dataset and store numpy array\n",
    "    dset = hf.create_dataset('my_data',(200,), dtype=int)\n",
    "    dset[...] = np.arange(200)\n",
    " \n",
    "    grp = hf.create_group('group1')\n",
    "    #add some metadata to group\n",
    "    grp.attrs['name'] = 'main group'\n",
    "    grp.attrs['author'] = 'ksopyla'\n",
    "    #create dataset in group1\n",
    "    train = np.random.random([100,100])\n",
    "    seg_ds = grp.create_dataset('train',data=train)\n",
    "    \n",
    "    \n",
    "    #we can easily create nested groups\n",
    "    sub_grp = grp.create_group('subgroup1/subgroup11')\n",
    "    ones_arr = np.ones((250,5000))\n",
    "    sub_ds = sub_grp.create_dataset('sensors', data=ones_arr)\n",
    "    sub_ds.attrs['sensor type'] = 'sensor IO'\n",
    "    sub_ds.attrs['date_taken'] = dt.datetime.now().isoformat()\n",
    "    \n",
    " \n",
    " \n",
    "with  h5py.File('file.hdf5','r') as hf:\n",
    " \n",
    "    # show all objects in file\n",
    "    hf.visit(printname)\n",
    "    \n",
    "    #get data set\n",
    "    dset = hf.get('my_data')\n",
    "    print(dset[0:10])\n",
    "    \n",
    "    #get data set, another way\n",
    "    dset2 = hf['my_data']\n",
    "    print(dset2[30:50])\n",
    "    \n",
    "    \n",
    "    #get gruoup\n",
    "    grp = hf['group1']\n",
    "    grp.items()\n",
    "    \n",
    "    #iterate over attributes\n",
    "    for item in grp.attrs.keys():\n",
    "        print (item + \":\", grp.attrs[item])\n",
    "    \n",
    "    \n",
    "    print(hf.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-cb3f6b967053>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-cb3f6b967053>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    with h5py.File('large.hdf5','w') as hf\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime as dt\n",
    " \n",
    "def printname(name):\n",
    "        print( name)\n",
    " \n",
    "CHUNK = 100\n",
    "MAX_CHUNKS=5000\n",
    "ARR_SIZE = 1000\n",
    " \n",
    "row_count = CHUNK\n",
    " \n",
    " \n",
    "kbytes_write = CHUNK*ARR_SIZE*4*(MAX_CHUNKS/1024.0)\n",
    "print('Write {} KB, {} MB'.format(kbytes_write, kbytes_write/1024))\n",
    " \n",
    "start = dt.datetime.now()\n",
    "with h5py.File('large.hdf5','w') as hf:\n",
    "    dset = hf.create_dataset('data',\\\n",
    "                        (CHUNK,ARR_SIZE),\\\n",
    "                        maxshape=(None,ARR_SIZE), chunks=True)\n",
    "    \n",
    "    #write first chunk\n",
    "    arr = np.ones((CHUNK, ARR_SIZE))\n",
    "    dset[:]=arr\n",
    "    \n",
    "    #expand dataset and write next chunks in to the file\n",
    "    for i in range(MAX_CHUNKS):\n",
    "        \n",
    "        arr = (i+2)*np.ones((CHUNK, ARR_SIZE))\n",
    "        rows = arr.shape[0]    \n",
    "            \n",
    "        dset.resize(row_count+rows, axis=0)\n",
    "        \n",
    "        dset[row_count:]=arr\n",
    "        \n",
    "        row_count+= rows\n",
    " \n",
    "duration = dt.datetime.now() - start\n",
    "throughput = kbytes_write/(1024*duration.total_seconds())\n",
    "print('Writing takes {} {}MB/s'.format(duration, throughput))\n",
    "    \n",
    "with h5py.File('large.hdf5','r') as hf\n",
    " hf.visit(printname)\n",
    " dset = hf.get('data')\n",
    " \n",
    " \n",
    " a = np.zeros((100,100))\n",
    " dset.read_direct(a, np.s_[0:10,0:10],np.s_[0:10,0:10])\n",
    " print(dset[80:110])\n",
    " print(hf.keys())\n",
    " print(dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
